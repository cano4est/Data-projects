{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trabajo Final 2024-02-Grupo4-Bigdata**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importacion de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mysql-connector-python in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (9.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexión con Fuente de Datos MySQL donde se encuentran las tablas de empleados y de ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   empleado_id  nombre   apellido      puesto fecha_contratacion  salario\n",
      "0            1  Carlos   Martínez    Vendedor         2023-02-15   3200.0\n",
      "1            2   Laura  Rodríguez  Supervisor         2022-08-10   4000.0\n",
      "2            3   Pedro      López     Gerente         2021-11-05   5000.0\n",
      "   venta_id  empleado_id   producto  cantidad  precio_unitario fecha_venta\n",
      "0         1            1     Tablet         5            150.0  2023-09-01\n",
      "1         2            2    Monitor         3            200.0  2023-09-10\n",
      "2         3            3    Teclado        10             30.0  2023-09-15\n",
      "3         4            1  Impresora         2            120.0  2023-09-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_10404\\1372409972.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, connection_mysql)\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar la conexión con la base de datos MySQL\n",
    "connection_mysql = mysql.connector.connect(\n",
    "   host='localhost',  \n",
    "    user='root',     \n",
    "    password='Nombre de usuario',  \n",
    "    database='Nombre de la base de datos'  \n",
    ")\n",
    "\n",
    "# Función para extraer datos de MySQL\n",
    "def extract_data_from_mysql(query):\n",
    "    return pd.read_sql(query, connection_mysql)\n",
    "\n",
    "# Extracción de datos de las tablas\n",
    "empleados = extract_data_from_mysql(\"SELECT * FROM empleados\")\n",
    "ventas = extract_data_from_mysql(\"SELECT * FROM ventas\")\n",
    "\n",
    "# Vista previa de los datos extraídos\n",
    "print(empleados.head())\n",
    "print(ventas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacion de Datos (Eliminación de espacios, Convertir a mayusculas, eliminacion de caracteres especiales, eliminación de duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   venta_id  empleado_id   producto  cantidad  precio_unitario fecha_venta  \\\n",
      "0         1            1     TABLET         5            150.0  2023-09-01   \n",
      "1         2            2    MONITOR         3            200.0  2023-09-10   \n",
      "2         3            3    TECLADO        10             30.0  2023-09-15   \n",
      "3         4            1  IMPRESORA         2            120.0  2023-09-20   \n",
      "\n",
      "   nombre  apellido      puesto fecha_contratacion  salario  \n",
      "0  CARLOS   MARTNEZ    VENDEDOR         2023-02-15   3200.0  \n",
      "1   LAURA  RODRGUEZ  SUPERVISOR         2022-08-10   4000.0  \n",
      "2   PEDRO      LPEZ     GERENTE         2021-11-05   5000.0  \n",
      "3  CARLOS   MARTNEZ    VENDEDOR         2023-02-15   3200.0  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Función para limpiar cadenas de texto\n",
    "def clean_text(text):\n",
    "    # Eliminar espacios al inicio y al final\n",
    "    text = text.strip()\n",
    "    # Convertir todo a mayúsculas\n",
    "    text = text.upper()\n",
    "    # Eliminar caracteres especiales, permitiendo solo letras, números y espacios\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Aplicación de las transformaciones a la tabla 'empleados'\n",
    "def transform_empleados(empleados):\n",
    "    # Limpiar espacios, convertir a mayúsculas y eliminar caracteres especiales en columnas de texto\n",
    "    empleados['nombre'] = empleados['nombre'].apply(clean_text)\n",
    "    empleados['apellido'] = empleados['apellido'].apply(clean_text)\n",
    "    empleados['puesto'] = empleados['puesto'].apply(clean_text)\n",
    "    \n",
    "    # Eliminar registros duplicados basados en 'empleado_id'\n",
    "    empleados = empleados.drop_duplicates(subset=['empleado_id'])\n",
    "    \n",
    "    return empleados\n",
    "\n",
    "# Aplicación de las transformaciones a la tabla 'ventas'\n",
    "def transform_ventas(ventas):\n",
    "    # Limpiar espacios, convertir a mayúsculas y eliminar caracteres especiales en columnas de texto\n",
    "    ventas['producto'] = ventas['producto'].apply(clean_text)\n",
    "    \n",
    "    # Eliminar registros duplicados basados en 'venta_id'\n",
    "    ventas = ventas.drop_duplicates(subset=['venta_id'])\n",
    "    \n",
    "    return ventas\n",
    "\n",
    "# Ejecutacion de las transformaciones\n",
    "empleados_limpios = transform_empleados(empleados)\n",
    "ventas_limpias = transform_ventas(ventas)\n",
    "\n",
    "# Unión de las tablas transformadas (empleados y ventas)\n",
    "empleados_ventas_limpios = pd.merge(ventas_limpias, empleados_limpios, on='empleado_id', how='left')\n",
    "\n",
    "# vista previa de los datos transformados\n",
    "print(empleados_ventas_limpios.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de tablas a S3\n",
    "\n",
    "Se realiza conversión de las tablas de MySql a archivos Csv para su posterior carga en un Bucket de S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente de datos 1 Mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.35.24)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.24 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (1.35.24)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\windows\\appdata\\roaming\\python\\python312\\site-packages (from botocore<1.36.0,>=1.35.24->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.36.0,>=1.35.24->boto3) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\windows\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.24->boto3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos CSV creados exitosamente.\n",
      "Archivo 'empleados_limpios.csv' subido exitosamente a S3.\n",
      "Archivo 'ventas_limpias.csv' subido exitosamente a S3.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import boto3\n",
    "\n",
    "# Paso 1: Guardar los DataFrames en archivos CSV locales\n",
    "\n",
    "# Guardar la tabla 'empleados_limpios' en un archivo CSV\n",
    "empleados_limpios.to_csv('empleados_limpios.csv', index=False, sep=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL, encoding='utf-8')\n",
    "\n",
    "# Guardar la tabla 'ventas_limpias' en un archivo CSV\n",
    "ventas_limpias.to_csv('ventas_limpias.csv', index=False, sep=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL, encoding='utf-8')\n",
    "\n",
    "print(\"Archivos CSV creados exitosamente.\")\n",
    "\n",
    "# Paso 2: Subir los archivos CSV a AWS S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'Insertar el nombre del bucket'  # Nombre del bucket de S3\n",
    "ruta_s3_empleados = 'tablas/empleados_limpios.csv'\n",
    "ruta_s3_ventas = 'tablas/ventas_limpias.csv'\n",
    "\n",
    "try:\n",
    "    # Subir el archivo CSV 'empleados_limpios.csv' al bucket especificado en la ruta adecuada\n",
    "    s3.upload_file('empleados_limpios.csv', bucket_name, ruta_s3_empleados)\n",
    "    print(\"Archivo 'empleados_limpios.csv' subido exitosamente a S3.\")\n",
    "\n",
    "    # Subir el archivo CSV 'ventas_limpias.csv' al bucket especificado en la ruta adecuada\n",
    "    s3.upload_file('ventas_limpias.csv', bucket_name, ruta_s3_ventas)\n",
    "    print(\"Archivo 'ventas_limpias.csv' subido exitosamente a S3.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error al subir los archivos a S3:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuente de datos 2 csv local\n",
    "\n",
    "Ahora se sube la segunda fuente de datos para el datalake la cual corresponde a un csv almacenado en local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Función para transformar el DataFrame\n",
    "def transformar_csv(filepath):\n",
    "    # Leer el CSV desde la ruta local proporcionada\n",
    "    try:\n",
    "        # Cargar el archivo CSV en un DataFrame de pandas\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Archivo '{filepath}' leído correctamente. Previsualización de los datos:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Aplicar transformaciones: limpieza de texto\n",
    "        if 'nombre' in df.columns:\n",
    "            df['nombre'] = df['nombre'].apply(clean_text)\n",
    "        if 'apellido' in df.columns:\n",
    "            df['apellido'] = df['apellido'].apply(clean_text)\n",
    "        if 'puesto' in df.columns:\n",
    "            df['puesto'] = df['puesto'].apply(clean_text)\n",
    "        if 'producto' in df.columns:\n",
    "            df['producto'] = df['producto'].apply(clean_text)\n",
    "            \n",
    "        # Renombrar todas las columnas a mayúsculas\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "\n",
    "        # Eliminar duplicados basados en las claves necesarias\n",
    "        df = df.drop_duplicates()\n",
    "        print(\"Transformaciones aplicadas correctamente. Previsualización de datos transformados:\")\n",
    "        print(df.head())\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo CSV: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'C:\\Users\\Windows\\Downloads\\pedidos.csv' leído correctamente. Previsualización de los datos:\n",
      "   pedido_id  venta_id  empleado_id fecha_pedido  cantidad_productos\n",
      "0          1       101            1   2024-10-20                   5\n",
      "1          2       102            2   2024-10-21                   3\n",
      "2          3       103            1   2024-10-22                   8\n",
      "3          4       104            3   2024-10-23                   6\n",
      "4          5       105            2   2024-10-24                   4\n",
      "Transformaciones aplicadas correctamente. Previsualización de datos transformados:\n",
      "   PEDIDO_ID  VENTA_ID  EMPLEADO_ID FECHA_PEDIDO  CANTIDAD_PRODUCTOS\n",
      "0          1       101            1   2024-10-20                   5\n",
      "1          2       102            2   2024-10-21                   3\n",
      "2          3       103            1   2024-10-22                   8\n",
      "3          4       104            3   2024-10-23                   6\n",
      "4          5       105            2   2024-10-24                   4\n",
      "Datos transformados listos para usar y guardados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ruta local del archivo CSV\n",
    "ruta_archivo = \"C:\\\\Users\\\\Windows\\\\Downloads\\\\pedidos.csv\"\n",
    "\n",
    "# función de transformación para leer y limpiar el archivo\n",
    "df_transformado = transformar_csv(ruta_archivo)\n",
    "\n",
    "# ruta donde se desea guardar el archivo CSV transformado\n",
    "ruta_guardado = \"C:\\\\Users\\\\Windows\\\\OneDrive - Universidad EIA\\\\Especialización IA\\\\Big data e integración de datos masivos\\\\Trabajo final datalake\\\\finalv2\\\\pedidos_transformados.csv\"\n",
    "\n",
    "# transformación se realizó correctamente\n",
    "if df_transformado is not None:\n",
    "    # Guardar el DataFrame transformado como CSV en la ruta especificada\n",
    "    df_transformado.to_csv(ruta_guardado, index=False)\n",
    "    print(\"Datos transformados listos para usar y guardados correctamente.\")\n",
    "else:\n",
    "    print(\"Hubo un problema al transformar los datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'pedidos_transformados.csv' subido exitosamente a S3.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'Nombre del bucket'\n",
    "archivo_origen = 'pedidos_transformados.csv'\n",
    "archivo_destino = 'tablas/pedidos_transformados.csv'\n",
    "\n",
    "try:\n",
    "    # Subir el archivo CSV 'pedidos_transformados.csv' al bucket especificado en la ruta adecuada\n",
    "    s3.upload_file(archivo_origen, bucket_name, archivo_destino)\n",
    "    print(\"Archivo 'pedidos_transformados.csv' subido exitosamente a S3.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error al subir el archivo a S3:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conexión con snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexión con Snowflake (Query Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.12.3)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (43.0.3)\n",
      "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (24.2.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (2.9.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\windows\\appdata\\roaming\\python\\python312\\site-packages (from snowflake-connector-python) (24.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (4.11.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (3.16.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in c:\\users\\windows\\appdata\\roaming\\python\\python312\\site-packages (from snowflake-connector-python) (4.2.1)\n",
      "Requirement already satisfied: tomlkit in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snowflake-connector-python) (0.13.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0->snowflake-connector-python) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conector de Snowflake importado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "print(\"Conector de Snowflake importado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage creado exitosamente en Snowflake para el bucket S3.\n",
      "Conexión exitosa. Versión de Snowflake: 8.40.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Conectar a Snowflake\n",
    "    connection_snowflake = snowflake.connector.connect(\n",
    "        user='Usuario Snowflake',\n",
    "        password='Contraseña Snowflake',\n",
    "        account='id de cuenta',\n",
    "        warehouse='Nombre del Warehouse',\n",
    "        database='Nombre de la base de datos',\n",
    "        schema='PUBLIC'\n",
    "    )\n",
    "    \n",
    "    # Creación de un cursor para ejecutar consultas\n",
    "    cursor = connection_snowflake.cursor()\n",
    "    \n",
    "    # Crear un stage que apunte al bucket de S3\n",
    "    # Necesitarás las credenciales de AWS\n",
    "    aws_access_key_id = \"Aws access key\"\n",
    "    aws_secret_access_key = \"Aws Secret access key\"\n",
    "    \n",
    "    # Consulta para crear un stage externo\n",
    "    create_stage_query = f\"\"\"\n",
    "    CREATE OR REPLACE STAGE my_s3_stage\n",
    "    URL = 'Url del bucket en s3'\n",
    "    CREDENTIALS = (AWS_KEY_ID='{aws_access_key_id}' AWS_SECRET_KEY='{aws_secret_access_key}')\n",
    "    FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY = '\"' SKIP_HEADER = 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ejecutar la consulta para crear el stage\n",
    "    cursor.execute(create_stage_query)\n",
    "    print(\"Stage creado exitosamente en Snowflake para el bucket S3.\")\n",
    "    \n",
    "    # Ejecutar una consulta de prueba para verificar la conexión\n",
    "    cursor.execute(\"SELECT CURRENT_VERSION();\")\n",
    "    \n",
    "    # Resultado\n",
    "    result = cursor.fetchone()\n",
    "    print(f\"Conexión exitosa. Versión de Snowflake: {result[0]}\")\n",
    "    \n",
    "except snowflake.connector.errors.Error as e:\n",
    "    print(f\"Error al conectar a Snowflake: {e}\")\n",
    "    \n",
    "finally:\n",
    "    # Cierre del cursor y conexión\n",
    "    try:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection_snowflake:\n",
    "            connection_snowflake.close()\n",
    "    except NameError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de tablas de s3 a snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'empleados_limpios' creada exitosamente.\n",
      "Tabla 'ventas_limpias' creada exitosamente.\n",
      "Tabla 'pedidos_transformados' creada exitosamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Conectar a Snowflake\n",
    "    connection_snowflake = snowflake.connector.connect(\n",
    "        user='Usuario Snowflake',\n",
    "        password='Contraseña Snowflake',\n",
    "        account='id de cuenta',\n",
    "        warehouse='Nombre del Warehouse',\n",
    "        database='Nombre de la base de datos',\n",
    "        schema='PUBLIC'\n",
    "    )\n",
    "    \n",
    "    # Creación de un cursor para ejecutar consultas\n",
    "    cursor = connection_snowflake.cursor()\n",
    "    \n",
    "    # Crear la tabla empleados_limpios\n",
    "    create_empleados_table_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE empleados_limpios (\n",
    "        empleado_id INT,\n",
    "        nombre VARCHAR,\n",
    "        apellido VARCHAR,\n",
    "        puesto VARCHAR,\n",
    "        fecha_contratacion DATE,\n",
    "        salario FLOAT\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_empleados_table_query)\n",
    "    print(\"Tabla 'empleados_limpios' creada exitosamente.\")\n",
    "    \n",
    "    # Crear la tabla ventas_limpias\n",
    "    create_ventas_table_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE ventas_limpias (\n",
    "        venta_id INT,\n",
    "        empleado_id INT,\n",
    "        producto VARCHAR,\n",
    "        cantidad INT,\n",
    "        precio_unitario FLOAT,\n",
    "        fecha_venta DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_ventas_table_query)\n",
    "    print(\"Tabla 'ventas_limpias' creada exitosamente.\")\n",
    "    \n",
    "    # Crear la tabla pedidos_transformados\n",
    "    create_pedidos_table_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE pedidos_transformados (\n",
    "        pedido_id INT,\n",
    "        venta_id INT,\n",
    "        empleado_id INT,\n",
    "        fecha_pedido DATE,\n",
    "        cantidad_productos INT\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_pedidos_table_query)\n",
    "    print(\"Tabla 'pedidos_transformados' creada exitosamente.\")\n",
    "    \n",
    "except snowflake.connector.errors.Error as e:\n",
    "    print(f\"Error al conectar a Snowflake o ejecutar consultas: {e}\")\n",
    "    \n",
    "finally:\n",
    "    # Cierre del cursor y conexión\n",
    "    try:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection_snowflake:\n",
    "            connection_snowflake.close()\n",
    "    except NameError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente en un workbench de Snowflake se crean los file format para el cargue de los csv desde S3 hasta Snowflake.\n",
    "\n",
    "De esta manera se completa la arquitectura de un Datalake con dos fuentes de datos diferentes aplicando transformaciones con python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE OR REPLACE FILE FORMAT csv_4\n",
    "TYPE = CSV\n",
    "FIELD_DELIMITER = ';'\n",
    "FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "SKIP_HEADER = 1\n",
    "NULL_IF = ('');\n",
    "\n",
    "CREATE OR REPLACE FILE FORMAT csv_3\n",
    "TYPE = CSV\n",
    "FIELD_DELIMITER = ','\n",
    "FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "SKIP_HEADER = 1\n",
    "NULL_IF = ('');\n",
    "\n",
    "-- Cargar datos en la tabla 'empleados_limpios'\n",
    "COPY INTO empleados_limpios\n",
    "FROM @final_bigdata/empleados_limpios.csv\n",
    "FILE_FORMAT = csv_4;\n",
    "\n",
    "-- Cargar datos en la tabla 'ventas_limpias'\n",
    "COPY INTO ventas_limpias\n",
    "FROM @final_bigdata/ventas_limpias.csv\n",
    "FILE_FORMAT = csv_4;\n",
    "\n",
    "-- Cargar datos en la tabla 'pedidos_transformados'\n",
    "COPY INTO pedidos_transformados\n",
    "FROM @final_bigdata/pedidos_transformados.csv\n",
    "FILE_FORMAT = csv_3;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
